# -*- coding: utf-8 -*-
"""עותק של MLCourse_KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19K-DR1t2FBKHTFBpWmtqEAdRSzhy_Uft

<h1><center><b>Machine Learning</></center></h1>
<h4><center>89-511</center></h4>
<h3><center>Exercise 1 - Practical Part</center></h3>

Your Name:  
<font color='red'>
Ido Aharon   
</font>

____________

In this exercise we will learn to:
1. Load and prepare datasets.
2. Implement KNN algorithm.

**Instructions:**
- Create a copy of this notebook and implement your code in it.
- Make sure all the cells run without errors.
- When you finish the assignment, download the notebook and submit it and your pdf file to the **"submit"** system. 
To Download the notebook go to **"File"** -> **"Download .ipynb"**.  



If you have questions, please ask at the forum on moodle.

Personal requests please send to TA emails.

### **1. Introduction to Colab and Numpy**

We use the Python programming language for all assignments in this course with the help of popular packages (like numpy, matplotlib, sklearn, scipy and pytorch).

Go over this **[numpy tutorial](https://numpy.org/doc/stable/user/quickstart.html)** or this **[tutorial](https://cs231n.github.io/python-numpy-tutorial/)** before starting this assignment and **[this notebook](https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb)** to understand how to work with basic packages and with Google Colab. It is important to go over them before starting this assignment.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import matplotlib.pyplot as plt
# %matplotlib inline

"""Play with NumPy here:"""

x = np.array([1,2,3])
y = np.array([4,5,6])
x, y

"""Pay attention: with NumPy, you can sum vectors in one line, you don't have to iterate over the elements."""

x + y
# x - y
# x * y
# x**2
# (x - y)**2

np.sum(x)
# np.mean(x)
np.sqrt(x)

"""### **2. Data**

##Data
We'll use CIFAR10 dataset.
It is a very familiar dataset. Please read a little bit [here](https://www.cs.toronto.edu/~kriz/cifar.html).

To import the dataset, we'll use the torch library. Don't dig about this library now, we will learn about it better in the next weeks.
"""

import torch
from torchvision import datasets,transforms
import torchvision

######################################################
##   Don't change this cell. Use it as a black box  ##
######################################################

transform = transforms.Compose(
    [transforms.ToTensor(),
     ])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,
                                          shuffle=True, num_workers=1)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=10,
                                         shuffle=False, num_workers=1)

# CIFAR10 labels:
classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

"""Let's see a few images with their labels"""

# functions to show an image
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# get some random training images
dataiter = iter(trainloader)
images, labels = dataiter.next()

testiter = iter(testloader)
test_images, test_labels = testiter.next()

# show images
imshow(torchvision.utils.make_grid(images[:6]))
# print labels
print(' '.join('%5s' % classes[labels[j]] for j in range(6)))

"""Each image is represented as a RGB image.
each pixel has a number between 0 and 1 on each dimension.
"""

images[0], images[0].size()

"""CIFAR is a big datadset, let's use only 100 images and convert them to numpy arrays

#### Train Set
"""

x_train = np.array(images)
y_train = np.array(labels)

x_train.shape, y_train.shape

"""#### Test Set"""

x_test = np.array(test_images)
y_test = np.array(test_labels)
x_test.shape, y_test.shape

"""### **3. KNN**

First of all, let's implement euclidean distance function, so we will be able to calculate closeness between images.

**A reminder**: the euclidean distance formula is:

![image.png](data:image/png;base64,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)

**Pay attention**: point representation might contains more than two numbers (means m>2).
"""

import math

def euclidean_distance(p1, p2):
  p3 = p1 - p2
  p3 = p3 * p3
  sum = p3.sum()
  sum = math.sqrt(sum)
  return sum

"""Now, let's implement KNeighborsClassifier class.

In 'fit' function you have to get the inputs, and prepare the classifier for prediction (think what exactly you have to prepare)

In 'predict' fucntion you have to get a test set and return a numpy array contains the classes (that the model predicted) of each element from the test set.

We added a few more function declarations that might help you. Fill free to implement them (or not) and to add some more functions.

In adition, we've presented a few KNN implementations in class. You are allowed to implement your own algorithm.

"""

class KNeighborsClassifier:
  def __init__(self, distance_metric, n_neighbors=None):
    self.k = n_neighbors
    self.metric = distance_metric
    self.images = None
    self.labels = None


  def fit(self, x, y):
    self.images = x
    self.labels = y

  def predict(self, x_test):
    answer = []
    for x in x_test:
      answer.append(self.predict_one(x))

    return np.array(answer)    

  def predict_one(self, x_point):
    neighbors = self.distance(x_point)
    neighbors = neighbors[0 : self.k] # slice only the first k neighbors
    return self.most_frequent(neighbors) # return the most common neighbor

  def most_frequent(self, List):
    counter = 0
    element = List[0]
     
    for i in List:
        curr_frequency = List.count(i)
        if(curr_frequency> counter):
            counter = curr_frequency
            element = i
 
    return element
  
  def distance(self, x_test):
    all_distances = []
    for x in self.images:
      all_distances.append(self.metric(x, x_test))

    merged = list(zip(all_distances, self.labels)) # merge the distances with the tags
    merged.sort()
    merged = list(zip(*merged))[1] # extract only the tags

    return merged

  def nearest_neighbor(self, x_test):
    return self.distance(x_test)[0]

def error_rate(y_predict, y_true):
  """
  y_predict: the model's output
  y_true: the expected output
  """
  return np.mean(y_predict != y_true)

"""**Using KNN**

Use the KNeighborsClassifier class you've implemented with n_neighbors=1. Fit this KNN model to the training data.
"""

knn = KNeighborsClassifier(euclidean_distance, n_neighbors=1)
knn.fit(x_train,y_train)

"""**Predictions and Evaluations**

Use the predict method to predict values using your KNN model and x_test. In addition, create a [confusion matrix](https://www.sciencedirect.com/topics/engineering/confusion-matrix) and a [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html).

Make sure you don't get any errors in this cell.
"""

pred = knn.predict(x_test)
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,pred))
print(classification_report(y_test,pred))

"""**Choosing a K Value**

Implement a for loop in 'find_best_k' function (hint: use KNeighborsClassifier object) that trains various KNN models with different k values, then keep track of the error_rate for each of these models with a list.
The function will return the best k, and the error rate of each k as a numpy array.
"""

MIN_K = 1
MAX_K = 15

def find_best_k(min_k=1, max_k=15):
  best_k = 0
  best_error = 1.0
  error_rates = []

  for k in range(min_k, max_k):
    knn = KNeighborsClassifier(euclidean_distance, k)
    knn.fit(x_train, y_train)

    predicate = knn.predict(x_test)
    rate = error_rate(y_test, predicate)

    if rate <= best_error:
      best_error = rate
      best_k = k

    error_rates.append(rate)

  return best_k, np.array(error_rates)

best_k, error_rate = find_best_k(min_k=MIN_K, max_k=MAX_K)

"""Now create the following plot using the information from your for loop."""

plt.figure(figsize=(10,6))
plt.plot(range(MIN_K,MAX_K),error_rate,color='blue',linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error rate vs K')
plt.xlabel('K')
plt.ylabel('Error rate')

"""**Retrain with new K Value**

Retrain your model with the best K value (up to you to decide what you want) and re-do the classification report and the confusion matrix.
"""

knn = KNeighborsClassifier(euclidean_distance, best_k)
knn.fit(x_train, y_train)
pred = knn.predict(x_test)

print(confusion_matrix(y_test,pred))
print(classification_report(y_test,pred))

"""**:)**"""